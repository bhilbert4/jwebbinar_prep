{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Mode Data Calibration\n",
    "\n",
    "**Date**: May 29, 2020\n",
    "\n",
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Pipeline Resources and Documentation](#resources)\n",
    "   * [Installation](#installation)\n",
    "   * [Reference Files](#reference_files)\n",
    "* [Imports](#Imports_ID)\n",
    "* [Convenience Functions](#convenience_functions)\n",
    "* [Download Data](#download_data)\n",
    "* [Methods for calling steps/pipelines](#calling_methods)\n",
    "   * [run() method](#run_method)\n",
    "   * [ASDF configuration files](#asdf_config)\n",
    "   * [call() method](#call_method)\n",
    "   * [command line](#command_line)\n",
    "* [calwebb_detector1](#detector1)\n",
    "* [calwebb_image2](#image2)\n",
    "* [calwebb_image3](#image3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">This is how you create a box around an important note.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    " The whole set of steps ran by TSOs by this first Stage of the CalWebb Pipeline can be found [<a href=\"https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html#calwebb-detector1\">here</a>]. In what follows, we'll have one section associated to each step, calibrating the data sequentially and exploring the outputs accordingly.\n",
    "\n",
    "There are several ways to call the pipeline.....(run() method, call() method, command line). We will show all three?? show all three for one step and from that point forward focus on only one??\n",
    "All step parameters have default values that the pipeline will use if the user does not provide values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "## Pipeline Resources and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different places to find information on installing and running the pipeline. This notebook will give a shortened description of the steps pulled from the detailed pipeline information pages, but to find more in-depth instructions use the links below.\n",
    "\n",
    "* [GitHub repository, with installation instructions](https://github.com/spacetelescope/jwst/blob/master/README.md)\n",
    "* [Pipeline documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/introduction.html)\n",
    "* [Help Desk](https://stsci.service-now.com/jwst?id=sc_cat_item&sys_id=27a8af2fdbf2220033b55dd5ce9619cd&sysparm_category=e15706fc0a0a0aa7007fc21e1ab70c2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='installation'></a>\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to install the pipeline is via `pip`. Below we show how to create a new conda environment, activate that environment, and then install the pipeline. For more detailed instructions, see the [installation instructions](https://github.com/spacetelescope/jwst/blob/master/README.md) on GitHub. You can name your environment anything you like. In the lines below, replace <env_name> with your chosen environment name.\n",
    "\n",
    ">`conda create -n <env_name> python`<br>\n",
    ">`conda activate <env_name>`<br>\n",
    ">`pip install jwst`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reference_files'></a>\n",
    "### Reference Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People at STScI should automatically have access to the Calibration Reference Data System (CRDS) cache for running the pipeline. For outside users, it is recommended to have the CRDS server download the reference files to your local system and use that local cache when running the pipeline. To do that, there are two environment variables that should be set prior to calling the pipeline. These are the CRDS_PATH and CRDS_SERVER_URL variables. In the example below, reference files will be downloaded to the \"crds_cache\" directory under the home directory.\n",
    "\n",
    ">`$ export CRDS_PATH=$HOME/crds_cache`<br>\n",
    ">`$ export CRDS_SERVER_URL=https://jwst-crds.stsci.edu`\n",
    "\n",
    "The first time you invoke the pipeline, the CRDS server should download all of the context and reference files that are needed for that pipeline run, and dump them into the CRDS_PATH directory. Subsequent executions of the pipeline will first look to see if it has what it needs in CRDS_PATH and anything it doesn't have will be downloaded from the STScI cache. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=#Imports_ID></a>\n",
    "## Imports\n",
    "\n",
    "Here are the lirbaries being imported here and why:\n",
    "\n",
    "- `numpy` for numerical calculations.\n",
    "- `matplotlib.pyplot` for plots.\n",
    "- `astropy.io.fits` for importing fits files.\n",
    "- `jwst.pipeline.calwebb_detector1` for using the CalWebb Detector 1 stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from astropy.io import fits\n",
    "from jwst.pipeline import calwebb_detector1\n",
    "from jwst.group_scale import GroupScaleStep\n",
    "from jwst.dq_init import DQInitStep\n",
    "from jwst.saturation import SaturationStep\n",
    "from jwst.superbias import SuperBiasStep\n",
    "from jwst.ipc import IPCStep                                                                                    \n",
    "from jwst.refpix import RefPixStep                                                                \n",
    "from jwst.linearity import LinearityStep\n",
    "from jwst.dark_current import DarkCurrentStep\n",
    "from jwst.jump import JumpStep\n",
    "from jwst.ramp_fitting import RampFitStep\n",
    "from jwst.gain_scale import GainScaleStep\n",
    "from jwst import datamodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version of the pipeline we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jwst\n",
    "print(jwst.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='convenience_functions'></a>\n",
    "## Define convenience functions and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions that we will use repeatedly throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files created in this notebook will be saved in a subdirectory\n",
    "# of the current working directory called `output`\n",
    "output_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(data):\n",
    "    \"\"\"Show the input image on the screen\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_by_side(data1, data2):\n",
    "    \"\"\"Show two images side by side for easy comparison\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='download_data'></a>\n",
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, directory='./'):\n",
    "    \"\"\"Download a file from a given URL\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL of the file to be downloaded\n",
    "        \n",
    "    directory : str\n",
    "        Directory into which the file should be saved\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download files from Box here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='calling_methods'></a>\n",
    "## Methods for calling steps/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three common methods by which the pipeline or pipeline steps can be called. From within python, the `run()` and `call()` methods can be used. Or, the `strun` command can be used from the command line. When using the `call()` method or `strun`, optional input parameters can be specified via [configuration files](#asdf_config). When using the `run()` method, these parameters are specified within python. See below for details on all three methods.\n",
    "\n",
    "For the pipeline/step calls in this notebook, we will show how to use all three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run_method'></a>\n",
    "### Run() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `run()` method, optional input parameters are specified using attributes of the pipeline or step class, rather than configuration files. \n",
    "[example usage of run() method](https://jwst-pipeline.readthedocs.io/en/stable/jwst/stpipe/call_via_run.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='asdf_config'></a>\n",
    "### ASDF Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling a pipeline or pipeline step using the call() method or the command line, \n",
    "\n",
    "show some examples here.\n",
    "\n",
    "[ASDF configuration file details](https://jwst-pipeline.readthedocs.io/en/stable/jwst/stpipe/config_asdf.html#config-asdf-files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='call_method'></a>\n",
    "### call() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example usage of call() method](https://jwst-pipeline.readthedocs.io/en/stable/jwst/stpipe/call_via_call.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='command_line'></a>\n",
    "### Command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example usage of command line calls](https://jwst-pipeline.readthedocs.io/en/stable/jwst/introduction.html?highlight=%22command%20line%22#running-from-the-command-line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='detector1'></a>\n",
    "## The calwebb_detector1 pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General comments about detector1 here. Inputs, Outputs, takes data from multiaccum ramps to slope images. Composed of multiple steps. Steps used/skipped are instrument-dependent. Data for all observation modes goes through calwebb_detector1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. [Run the entire pipeline](#detector1_at_once)\n",
    "1. [The `group_scale` step](#groupscale)\n",
    "2. [The `dq_init` step](#dq_init)\n",
    "3. [The `saturation` step](#saturation)\n",
    "4. [The `superbias` step](#superbias)\n",
    "5. [The `refpix` step](#refpix)\n",
    "6. [The `linearity` step](#linearity)\n",
    "7. [The `darkcurrent` step](#dc)\n",
    "8. [The `jump` step](#jump)\n",
    "9. [The `ramp_fitting` step](#ramp_fitting)\n",
    "10. [The `gain_scale` step](#gain_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='detector1_at_once'></a>\n",
    "## Run the entire `calwebb_detecor1` pipeline\n",
    "\n",
    "In this section we show how to run the entire calwebb_detector1 pipeline with a single call. We set parameter values for some of the individual steps, save some outputs, etc, and then call the pipeline.\n",
    "\n",
    "[Pipeline output suffixes](https://jwst-pipeline.readthedocs.io/en/stable/jwst/introduction.html#pipeline-step-suffix-definitions)\n",
    "\n",
    "In subsequent sections, we show how to run each step individually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector1_output_file = 'XXXXXX_rate.fits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the run() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the run() method\n",
    "detector1 = Detector1Pipeline()\n",
    "detector1.output_dir = output_dir\n",
    "detector1.save_results = True\n",
    "\n",
    "# Set some parameters for some of the steps, in\n",
    "# order to show how it's done\n",
    "detector1.refpix.use_side_ref_pix = True\n",
    "detector1.linearity.save_results = True\n",
    "detector1.jump.rejection_threshold = 6\n",
    "\n",
    "# Run the pipeline\n",
    "detector1.run(uncal_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the call() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the call() method - !!!!!need to go into cfg files here!!!!\n",
    "Detector1Pipeline.call(uncal_file, output_dir=output_dir, save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling from the command line\n",
    "# strun XXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary output of the calwebb_detector1 pipeline is a file containing a rate image for the exposure. The units of the data are ADU/sec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_file = uncal_file.replace('uncal.fits', 'rate.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_data = fits.getdata(rate_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(rate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since we set the `detector1.linearity.save_results` parameter to True in the call above, the pipeline saved the results of the linearity step. In this case, the output file will have the same name as the input uncal file, but with the suffix 'linearity' rather than 'uncal'. \n",
    "\n",
    "**NOTE:** This differs slightly from the case where we call the linearity step itself and save the results. In that case, the output file will have the suffix 'linearitystep' rather than 'linearty'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_file = uncal_file.replace('uncal.fits', 'linearity.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_data = fits.getdata(linear_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data in the final group of the linearized data:\n",
    "show(lin_data[0, -1, :. :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below we run the steps contained within calwebb_detector1 one at a time, in order to more clearly see what each step is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='groupscale'></a>\n",
    "## The `group_scale` step\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This step rescales pixel values in the raw JWST science products in cases where multiple [frames](https://jwst-docs.stsci.edu/understanding-exposure-times#UnderstandingExposureTimes-uptherampHowup-the-rampreadoutswork) were averaged on-board to create the [groups](https://jwst-docs.stsci.edu/understanding-exposure-times#UnderstandingExposureTimes-uptherampHowup-the-rampreadoutswork) in the multiaccum ramp, but the number of frames per group is not a power of 2. This occurs primarily in [NIRSpec IRS^2 data that uses the NRSIRS2 readout pattern (see Table 1)](https://jwst-docs.stsci.edu/near-infrared-spectrograph/nirspec-instrumentation/nirspec-detectors/nirspec-detector-readout-modes-and-patterns) data. Data with no frame averaging, or where the number of frames is a power of 2, will not be affected by this step. See Figure 2 in the [NIRCam detector readout](https://jwst-docs.stsci.edu/near-infrared-camera/nircam-instrumentation/nircam-detector-overview/nircam-detector-readout-patterns) page for some examples of readout patterns where multiple frames are averaged to create each group.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/group_scale/description.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There are no optional arguments for this step\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step does not use any reference files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the run() method\n",
    "group_scale = GroupScaleStep()\n",
    "group_scale.output_dir = output_dir\n",
    "group_scale.save_results = True\n",
    "group_scale.run(uncal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the output is saved, the group_scale step will\n",
    "# attach a suffix of 'group_scale' to the input filename.\n",
    "group_scale_output_file = os.path.join(output_dir, \n",
    "                                       uncal_file.replace('uncal.fits', 'groupscalestep.fits'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Call() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show default asdf configuration file here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the call() method - need to go into cfg files here...\n",
    "GroupScaleStep.call(uncal_file, output_dir=output_dir,\n",
    "                    output_file=group_scale_output_file, save_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling from the command line\n",
    "# strun group_scale.asdf jw00017001001_01101_00001_nrs1_uncal.fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our example file does not average frames into groups, the group_scale step will not change the data at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_data = fits.getdata(uncal_file)\n",
    "group_scale_data = fits.getdata(group_scale_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input uncal data and resulting output from the step (`output/data_groupscalestep.fits`) and compute the difference between the values from both. The expectation is that there will be no difference between the two since the input data do not create groups by averaging frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = uncal_data - group_scale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(difference.flatten() == 0.)[0]\n",
    "print('{} unchanged signal values out of {} measuremenents.'.format(len(idx), difference.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Move this cell and the cell below to a step where the header is changed, such as assign_wcs or flux_cal\n",
    "\n",
    "**Indeed, the step does not do anything. Both outputs are exactly equal, as expected**. What about the headers? To check this out, let's use `fits.HeaderDiff`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fits.HeaderDiff(hdul['SCI'].header, hdul_group_scale['SCI'].header)\n",
    "results.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='dq_init'> The `dq_init` step </a>\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The next step in Stage 1 is the `dq_init` step. This step populates the Data Quality (DQ) mask that is associated with the data file. The DQ flags from the `MASK` bad pixel are copied into the `PIXELDQ` extension of the input file. A table showing the [mapping of bit values](https://jwst-pipeline.readthedocs.io/en/stable/jwst/references_general/references_general.html#data-quality-flags) in the `MASK` file decribes what types of bad pixels can be flagged. Any other bad pixel types will be ignored.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/dq_init/description.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There are no optional arguments for this step\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step uses the `MASK` reference file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### run() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the run() method\n",
    "dq_init = DQInitStep()\n",
    "dq_init.output_dir = 'output'\n",
    "dq_init.save_results = True\n",
    "\n",
    "# Note that you can call the run() method using EITHER:\n",
    "\n",
    "# the datamodel instance from the previously-run\n",
    "# group_scale step\n",
    "dq_init.run(group_scale)\n",
    "\n",
    "# OR the output file from the group_scale step\n",
    "# BE CAREFUL WITH THIS. PREVIOUS PIPELINE STEPS CAN ATTACH SUFFIXES TO THE PROVIDED\n",
    "# OUTPUT FILE NAME, IF THE PROVIDED NAME DOESN'T END WITH THE RECOGNIZED SUFFIX.\n",
    "# THEREFORE, THE ACTUAL SAVED FILENAME MAY DIFFER FROM WHAT YOU REQUESTED.\n",
    "dq_init.run(group_scale_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### call() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the call() method - need to go into cfg files here...\n",
    "\n",
    "# also show call using the datamodel or the filename\n",
    "\n",
    "DQInitStep.call('output/data_k2-141_groupscalestep.fits', output_dir='output',save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call from the command line\n",
    "# strun XXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step finished without crashing, but as it is said above, there are some errors and warnings worth noting:\n",
    "1. There is a CRDS ERROR \"Error determining best reference for 'pars-dqinitstep'  =   Unknown reference type 'pars-dqinitstep'\"\n",
    "2. `CDP_WARM` and `CDP_NOISY` do not correspond to existing `DQ` mnemonics, so they are ignored (this is normal, see below).\n",
    "3. There is also a WARNING with the `T_SECONDARY` Keyword. It appears it is greater than 8 characters, or contains characters not allowed by the FITS standard. Also normal (at least we see it with other steps as well).\n",
    "\n",
    "Let's take a look at the `hdul`'s of both the uncal data and the `dqinit` products:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values in the `SCI` extension are not changed in this step. Instead, the DQ flags are copied into the `PIXELDQ` extension. The `GROUPDQ` values are not changed in this step. Let's check the `PIXELDQ` values and see what has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_init_output_file = os.path.join(output_dir, uncal_file.replace('uncal.fits', 'dqinitstep.fits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_scale_pixeldq = fits.getdata(group_scale_output_file, 'PIXELDQ')\n",
    "dq_init_pixeldq = fits.getdata(dq_init_output_file, 'PIXELDQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_pixelDQ = dq_init_pixeldq - group_scale_pixeldq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pixelDQ = np.where(difference_pixelDQ.flatten() == 0.)[0]\n",
    "print('Total pixels in PIXELDQ: {}'.format(difference_pixelDQ.size))\n",
    "print('{} pixels did not change.'.format(len(idx_pixelDQ)))\n",
    "print('{} pixels did change.'.format(difference_pixelDQ.size - len(idx_pixelDQ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the unique DQ values in the `PIXELDQ` array. Note that unrecognized bad pixel types (such as XXXXXX) were not copied over to the `PIXELDQ` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dq_vals = np.unique(dq_init_pixeldq)\n",
    "print(unique_dq_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show here that there are no XXXX flags, where XXXX is for an unrecognized bad pixel type in the mask reffile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the pipeline product is much less populated. This is because the `CDP_WARM` and `CDP_NOISY` flags are not propagated, as these are not recognized by the pipeline explicitly. These are just \"extra\" flags from the reference files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='saturation'> The `saturation` step </a>\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This step checks the signal values in all pixels across all groups, and adds a [`saturated` flag](https://jwst-pipeline.readthedocs.io/en/stable/jwst/references_general/references_general.html#data-quality-flags) to the `GROUPDQ` extension for pixels and groups where the signal is above the saturation limit.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/saturation/description.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There are no optional arguments for this step\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step uses the [`SATURATION`](https://jwst-pipeline.readthedocs.io/en/stable/jwst/saturation/reference_files.html) reference file. This file contains a map of the saturation threshold in ADU for each pixel on the detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### run() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the run() method\n",
    "saturation = SaturationStep()\n",
    "saturation.output_dir = 'output'\n",
    "saturation.save_results = True\n",
    "\n",
    "# Note that you can call the run() method using EITHER:\n",
    "\n",
    "# the datamodel instance from the previously-run\n",
    "# dq_init step\n",
    "saturation.run(dq_init)\n",
    "\n",
    "# OR the output file\n",
    "saturation.run(dq_init_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### call() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaturationStep.call('output/data_k2-141_dqinitstep.fits', output_dir=output_dir,\n",
    "                    save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strun XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step finished without crashing, but as with the `dq_init` step, there are some errors and warnings worth noting:\n",
    "1. There is a CRDS ERROR \"Error determining best reference for 'pars-saturationstep'  =   Unknown reference type 'pars-saturationstep'\"\n",
    "2. The same warning about `T_SECONDARY` Keyword shows up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any saturated values, they should appear in the `GROUPDQ` arrays. Let's examine the `GROUPDQ` data and see if there are any detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_output_file = os.path.join(output_dir,\n",
    "                                      uncal_file.replace('uncal.fits', 'saturationsstep.fits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_groupdq = fits.getdata(saturation_output_file, 'GROUPDQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturated = np.where(saturation_groupdq & dqflags.pixel['SATURATED'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sat_flags = len(saturated[0])\n",
    "print(('Found {} saturated flags. This may include multiple saturated '\n",
    "       'groups in a given pixel'.format(num_sat_flags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the DQ flags for one of these pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_int, sat_group, sat_y, sat_x = saturated\n",
    "sat_index = int(len(sat_y) / 2)\n",
    "print(saturation_groupdq[sat_int[sat_index], :, sat_y[sat_index], sat_x[sat_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pixel saturated in group XX, and is flagged as saturated from that group to the end of the integration. This means that in the linearity correction step later, groups XX - XX will be ignored and not corrected. They will also be ignored in the jump step, and the ramp-fitting step at the end of calwebb_detector1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_reffile = fits.getheader(saturation_output_file)['R_SATURA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to find CRDS_CACHE directory so that we can load the saturation reference file and look at the threshold for this pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='superbias'> </a>\n",
    "## The `superbias` step\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This step substracts the superbias reference frame from each group of the science exposure.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "[Full description](https://jwst-pipeline.readthedocs.io/en/stable/jwst/superbias/description.html) of the step.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "There are no optional arguments for this step\n",
    "\n",
    "#### Reference files used\n",
    "\n",
    "This step uses the [`SUPERBIAS`](https://jwst-pipeline.readthedocs.io/en/stable/jwst/superbias/reference_files.html) reference file. This file contains a map of the superbias signal in ADU for each pixel on the detector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### run() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### call() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SuperBiasStep.call('output/data_k2-141_saturationstep.fits', output_dir='output',\n",
    "                   save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Command line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "strun XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare how the science products visually look like in comparison with the raw `uncal` data for the last group of the first integration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superbias_output_file = os.path.join(output_dir, uncal_file.replace('uncal.fits', 'superbiasstep.fits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(uncal_data, superbias_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='refpix'> The `refpix` step </a>\n",
    "\n",
    "This step uses the reference pixels in order to remove some extra counts added around due to readout electronics. These reference pixels are 4-pixel wide strip around the edge of the detector that don't detect light, so they serve to measure these effects. Let's apply this step to the superbias products obtained before (`refpix` assumes this step has been carried out):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calwebb_detector1.refpix_step.RefPixStep.call('output/data_k2-141_superbiasstep_corrected.fits', output_dir='output',save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul_refpix = fits.open('output/data_k2-141_superbiasstep_corrected_refpixstep.fits')\n",
    "print(hdul_refpix.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, there are some changes. It is important to note here, that the reference pixels of our simulations are zeroes, so in theory this step shouldn't do nothing. Let's see what changed in the last group of the first integration, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Superbias corrected image (pipeline product):')\n",
    "im = plt.imshow(hdul_superbias['SCI'].data[0,2,:,:] - hdul_refpix['SCI'].data[0,2,:,:])\n",
    "im.set_clim(-100,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='linearity'> The `linearity` step </a>\n",
    "\n",
    "This step applies the [classical linearity correction](https://jwst-pipeline.readthedocs.io/en/latest/jwst/linearity/description.html) to the data on a pixel-by-pixel, integration-by-integration, group-by-group manner. Let's apply it to our saturation-flagged (real) products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calwebb_detector1.linearity_step.LinearityStep.call('output/data_k2-141_superbiasstep_corrected_refpixstep.fits', output_dir='output',save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, quick exploration of the products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul_linearity = fits.open('output/data_k2-141_superbiasstep_corrected_linearitystep.fits')\n",
    "print(hdul_linearity.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good! To see how well this is being done, let's once again use the third group of the first integration as a reference. Let's use the NIRISS linearity reference file used by the pipeline to run the corrections ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients for our subarray:\n",
    "linearity_ref = fits.open('/grp/crds/cache/references/jwst/jwst_niriss_linearity_0011.fits')\n",
    "coeffs = linearity_ref['COEFFS'].data[:,-256:,:]\n",
    "\n",
    "# Extract third group of first integration before linearity correction:\n",
    "third_group = hdul_refpix['SCI'].data[0,2,:,:]\n",
    "third_group_corrected = np.zeros(third_group.shape)\n",
    "\n",
    "# Correct for linearity pixel-to-pixel:\n",
    "for i in range(third_group.shape[0]):\n",
    "    for j in range(third_group.shape[1]):\n",
    "        third_group_corrected[i,j] = np.polyval(coeffs[::-1,i,j],third_group[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Linearity correction (our) - Linearity correction (pipeline product):')\n",
    "difference_linearity = third_group_corrected - hdul_linearity['SCI'].data[0,2,:,:]\n",
    "im = plt.imshow(difference_linearity)\n",
    "im.set_clim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nonzero_values = np.where(difference_linearity.flatten() != 0)[0]\n",
    "print(len(nonzero_values),'pixels not identical. All the rest give the same result.')\n",
    "print('(Flattened) Positions:',nonzero_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 19 non-identical pixels were expected (in fact --- we expected 20!). Remember that, above, we discovered 20 saturated pixels were identified on each group --- all but one of them, as expected, were not corrected by the pipeline (but were by our code). The missing pixel, however, is at (flattened) pixel position 162970 (see the [saturation](#saturation) section). That pixel is a bit of a special fella. Let's see what its original value was, and what its corrected value looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 162970\n",
    "print('Original value:',third_group.flatten()[index])\n",
    "print('Our corrected value:',third_group_corrected.flatten()[index])\n",
    "print('Pipeline corrected value:',hdul_linearity['SCI'].data[0,2,:,:].flatten()[index])\n",
    "print('Coefficients:',coeffs.reshape(6,len(third_group.flatten()))[:,index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That pixel has not only a negative value, but the linearity correction coefficients are basically all zeroes (or very small values, judging from the rest of the coefficients). The only coefficient that is not zero and equal to one is the second, which is the linear coefficient in the polynomial expansion of the linearity correction ($c_0 + c_1F +c_2F^2 ....$, so $c_1$ in this case). This gives this pixel a corrected value equal to its input value, and hence why it was detected as \"corrected by the pipeline\" with our search above. In reality, the pixel was not touched by the pipeline (as expected, because is marked as saturated), but because our correction gave back the same input value, we didn't detected it above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these results make total sense, <font color='green'>**we consider the step validated from the NIRISS/SOSS point of view.**</font>\n",
    "\n",
    "## <a id='dc'> The `darkcurrent` step </a>\n",
    "\n",
    "This simply substract the dark current signals. Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calwebb_detector1.dark_current_step.DarkCurrentStep.call('output/data_k2-141_superbiasstep_corrected_linearitystep.fits', output_dir='output',save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul_dark = fits.open('output/data_k2-141_superbiasstep_corrected_darkcurrentstep.fits')\n",
    "print(hdul_dark.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good. Let's see how much changed for the third group on the first integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference file for darks:\n",
    "darkcurrent = fits.open('/grp/crds/cache/references/jwst/jwst_niriss_dark_0114.fits')\n",
    "print(darkcurrent['SCI'].data.shape)\n",
    "\n",
    "# Plot:\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Linearity-corrected data (i.e., no dark-correction):')\n",
    "im = plt.imshow(hdul_linearity['SCI'].data[0,2,:,:])\n",
    "im.set_clim(-1000,1000)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Dark-frame corrected data:')\n",
    "im = plt.imshow(hdul_dark['SCI'].data[0,2,:,:])\n",
    "im.set_clim(-1000,1000)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Difference:')\n",
    "im = plt.imshow(hdul_dark['SCI'].data[0,2,:,:] - hdul_linearity['SCI'].data[0,2,:,:])\n",
    "im.set_clim(-10,10)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Difference minus dark frame in third group:')\n",
    "im = plt.imshow(hdul_dark['SCI'].data[0,2,:,:] - hdul_linearity['SCI'].data[0,2,:,:] + darkcurrent['SCI'].data[2,:,:])\n",
    "im.set_clim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected behaviour --- we can see evident, small, vertical strips of signal being removed; these come from the 1/f components of the noise, showing up in the columns as expected for NIRISS/SOSS. The last plot shows that the pipeline is doing what is expected: removing, for the second group, the second dark frame (which is the second group as well of the reference data itself) of the set of 50 available to remove. Given this shows the expected behaviour, <font color='green'>**we consider the step validated from the NIRISS/SOSS point of view.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='jump'> The `jump` step </a>\n",
    "\n",
    "This step detects and flags jumps in the ramps. Let's go ahead and run it on our dark-frame corrected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calwebb_detector1.jump_step.JumpStep.call('output/data_k2-141_superbiasstep_corrected_darkcurrentstep.fits', output_dir='output',save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul_jump = fits.open('output/data_k2-141_superbiasstep_corrected_jumpstep.fits')\n",
    "print(hdul_jump.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good. Now, let's check the `GROUPDQ` of the third group of the first integration, to see if there are any differences, which we will attribute to the jump step detecting jumps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_dark_jump = hdul_dark['GROUPDQ'].data[0,2,:,:] - hdul_jump['GROUPDQ'].data[0,2,:,:]\n",
    "idx_dark_jump = np.where(diff_dark_jump!=0)\n",
    "print(len(idx_dark_jump[0]),'detected jumps out of {0:} pixels (i.e., {1:.1f} percent of pixels in group 3)!'.format(2048*256,100*len(idx_dark_jump[0])/(2048*256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a lot of jumps. Let's plot some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = idx_dark_jump[0],idx_dark_jump[1]\n",
    "for i in range(len(x)):\n",
    "    idx = np.where(hdul_jump['GROUPDQ'].data[0,:,x[i],y[i]] == 4)[0]\n",
    "    plt.errorbar([0,1,2],hdul_jump['SCI'].data[0,:,x[i],y[i]],yerr=hdul_jump['ERR'].data[0,:,x[i],y[i]],fmt='-.')\n",
    "    plt.plot(idx,hdul_jump['SCI'].data[0,idx,x[i],y[i]],'o')\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Most likely this is due to the errorbars not being calculated correctly to detect the jumps (due to mismatches between the reference files used to generate the data and the ones used by the pipeline, see below for details). We'll revisit this step once this is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ramp_fitting'> The `ramp_fitting` step </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step fits a ramp to the data. Let's go ahead and run it on our jump-product data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calwebb_detector1.ramp_fit_step.RampFitStep.call('output/data_k2-141_superbiasstep_corrected_jumpstep.fits', output_dir='output',save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems normal. An important detail here is that there are two outputs: output `*_0_rampfitstep.fits` is the classic \"rate\" output which weights all the ramps for each integration (i.e., it is a 2D product). Output `*_1_rampfitstep.fits` are the \"rateints\" products, the slopes of each integration separately. Let's load the products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul_rate = fits.open('output/data_k2-141_superbiasstep_corrected_0_rampfitstep.fits')\n",
    "print(hdul_rate.info())\n",
    "hdul_rateints = fits.open('output/data_k2-141_superbiasstep_corrected_1_rampfitstep.fits')\n",
    "print(hdul_rateints.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good. Note errors have been added to the ramps (`ERR`), as well as `INT_TIMES` (which should save the timestamps of the observations), and variances. To test this step, let's do our own ramp fitting on the `output/data_k2-141_superbiasstep_corrected_jumpstep.fits` products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot:\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Pipeline ramp-fitting:')\n",
    "im = plt.imshow(hdul_rateints['SCI'].data[0,:,:])\n",
    "im.set_clim(-100,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(hdul_rateints['SCI'].data[0,:,1000],label='Pipeline product')\n",
    "plt.legend()\n",
    "plt.ylabel('Counts/second')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gain_scale'> The `gain_scale` step </a>\n",
    "\n",
    "We note this step only applies to NIRSpec, so it should be skipped. Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calwebb_detector1.gain_scale_step.GainScaleStep.call('output/data_k2-141_superbiasstep_corrected_1_rampfitstep.fits', output_dir='output',save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is properly skipped (really because there is no `GAINFACT` in the headers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
